# from pyspark.sql import SparkSession
# from pyspark.sql.types import (
#     StringType, DecimalType, IntegerType, StructType, StructField
# )
#
# # åˆå§‹åŒ–SparkSessionï¼Œé…ç½®HiveåŠ¨æ€åˆ†åŒºä¸ºéä¸¥æ ¼æ¨¡å¼ï¼ˆè§£å†³æ ¸å¿ƒé”™è¯¯ï¼‰
# spark = SparkSession.builder \
#     .appName("InsertDataToAdsTable") \
#     .config("spark.sql.catalogImplementation", "hive") \
#     .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
#     .config("hive.exec.dynamic.partition", "true")   \
#     .config("hive.exec.dynamic.partition.mode", "nonstrict")   \
#     .enableHiveSupport() \
#     .getOrCreate()
#
# # CSVæ–‡ä»¶è·¯å¾„ï¼ˆæ›¿æ¢ä¸ºå®é™…è·¯å¾„ï¼‰
# csv_path = "ads_product_value_evaluation_10000.csv"
#
# # å®šä¹‰ä¸è¡¨ç»“æ„ä¸€è‡´çš„Schemaï¼ˆåŒ¹é…æ–‡æ¡£ADSå±‚å­—æ®µï¼‰
# schema = StructType([
#     StructField("product_id", StringType(), nullable=False),
#     StructField("product_name", StringType(), nullable=True),
#     StructField("stat_date", StringType(), nullable=False),  # åŠ¨æ€åˆ†åŒºå­—æ®µ
#     StructField("traffic_acquisition_score", DecimalType(5, 2), nullable=True),
#     StructField("traffic_conversion_score", DecimalType(5, 2), nullable=True),
#     StructField("content_marketing_score", DecimalType(5, 2), nullable=True),
#     StructField("customer_acquisition_score", DecimalType(5, 2), nullable=True),
#     StructField("service_quality_score", DecimalType(5, 2), nullable=True),
#     StructField("total_score", DecimalType(5, 2), nullable=True),
#     StructField("level", StringType(), nullable=True),
#     StructField("amount", DecimalType(12, 2), nullable=True),
#     StructField("price", DecimalType(8, 2), nullable=True),
#     StructField("sales_num", IntegerType(), nullable=True),
#     StructField("visitor_num", IntegerType(), nullable=True),
#     StructField("visitor_sales_ratio", DecimalType(5, 2), nullable=True)
# ])
#
# # è¯»å–CSVæ•°æ®
# df = spark.read \
#     .format("csv") \
#     .option("header", "true") \
#     .schema(schema) \
#     .load(csv_path)
#
# # å†™å…¥æ•°æ®ï¼šé€‚é…HiveåŠ¨æ€åˆ†åŒºï¼Œç¬¦åˆæ–‡æ¡£å­˜å‚¨è¦æ±‚
# df.write \
#     .mode("append") \
#     .format("hive")  \
#     .partitionBy("stat_date")   \
#     .saveAsTable("gd.ads_product_value_evaluation")
#
# # éªŒè¯æ•°æ®æ’å…¥ç»“æœ
# count = spark.sql("SELECT COUNT(*) FROM gd.ads_product_value_evaluation").collect()[0][0]
# print(f"æˆåŠŸæ’å…¥{count}æ¡æ•°æ®åˆ°gd.ads_product_value_evaluationè¡¨")
#
# spark.stop()





















from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StringType, DecimalType, IntegerType, StructType, StructField
)

# åˆå§‹åŒ–SparkSessionï¼Œé€‚é…Hiveæ•°ä»“ç¯å¢ƒ
spark = SparkSession.builder \
    .appName("CreateAndInsertAdsProductValueEvaluation") \
    .config("spark.sql.catalogImplementation", "hive") \
    .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
    .config("hive.exec.dynamic.partition", "true")   \
    .config("hive.exec.dynamic.partition.mode", "nonstrict")   \
    .enableHiveSupport() \
    .getOrCreate()

# 1. å…ˆåœ¨Hiveä¸­åˆ›å»ºè¡¨ï¼ˆç¬¦åˆæ–‡æ¡£å…¨å“ä»·å€¼è¯„ä¼°çš„ADSå±‚è®¾è®¡ï¼‰
create_table_sql = """
CREATE TABLE IF NOT EXISTS gd.ads_product_value_evaluation (
    product_id STRING COMMENT 'å•†å“å”¯ä¸€æ ‡è¯†ID',
    product_name STRING COMMENT 'å•†å“åç§°',
    traffic_acquisition_score DECIMAL(5,2) COMMENT 'æµé‡è·å–ç»´åº¦è¯„åˆ†',
    traffic_conversion_score DECIMAL(5,2) COMMENT 'æµé‡è½¬åŒ–ç»´åº¦è¯„åˆ†',
    content_marketMENT 'å®¢æˆ·æ‹‰æ–°ç»´åº¦è¯„åˆ†',
    service_quality_score DECIMAL(5,2) COMMENT 'ing_score DECIMAL(5,2) COMMENT 'å†…å®¹è¥é”€ç»´åº¦è¯„åˆ†',
    customer_acquisition_score DECIMAL(5,2) COMæœåŠ¡è´¨é‡ç»´åº¦è¯„åˆ†',
    total_score DECIMAL(5,2) COMMENT 'ç»¼åˆè¯„åˆ†ï¼ˆåŸºäºäº”ç»´åº¦è®¡ç®—ï¼‰',
    level STRING COMMENT 'è¯„åˆ†ç­‰çº§ï¼ˆA/B/C/Dçº§ï¼Œ85+â†’Açº§ï¼Œ70-85â†’Bçº§ï¼Œ50-70â†’Cçº§ï¼Œ<50â†’Dçº§ï¼‰',
    amount DECIMAL(12,2) COMMENT 'é”€å”®é¢ï¼ˆé‡‘é¢ç»´åº¦ï¼Œç”¨äºè¯„åˆ†-é‡‘é¢åˆ†æï¼‰',
    price DECIMAL(8,2) COMMENT 'å•ä»·ï¼ˆä»·æ ¼ç»´åº¦ï¼Œç”¨äºä»·æ ¼-é”€é‡åˆ†æï¼‰',
    sales_num INT COMMENT 'é”€é‡ï¼ˆç”¨äºä»·æ ¼-é”€é‡/è®¿å®¢-é”€é‡åˆ†æï¼‰',
    visitor_num INT COMMENT 'è®¿å®¢æ•°ï¼ˆç”¨äºè®¿å®¢-é”€é‡åˆ†æï¼‰',
    visitor_sales_ratio DECIMAL(5,2) COMMENT 'è®¿å®¢-é”€é‡æ¯”ï¼ˆé”€é‡/è®¿å®¢æ•°ï¼Œè¯„ä¼°è®¿å®¢è½¬åŒ–æ•ˆç‡ï¼‰'
)
PARTITIONED BY (stat_date STRING COMMENT 'ç»Ÿè®¡æ—¥æœŸï¼ŒæŒ‰å¤©åˆ†åŒº')
STORED AS PARQUET
COMMENT 'å…¨å“ä»·å€¼è¯„ä¼°ADSå±‚è¡¨ï¼Œä»»åŠ¡ç¼–å·ï¼šå¤§æ•°æ®-ç”µå•†æ•°ä»“-07-å•†å“ä¸»é¢˜å•†å“è¯Šæ–­çœ‹æ¿ï¼Œæ”¯æŒåŸºäºè¯„åˆ†-é‡‘é¢/ä»·æ ¼-é”€é‡/è®¿å®¢-é”€é‡ç»´åº¦çš„å•†å“ä»·å€¼åˆ†æğŸ”¶1-20ğŸ”¶ğŸ”¶1-21ğŸ”¶ğŸ”¶1-26ğŸ”¶'
"""
spark.sql(create_table_sql)

# 2. è¯»å–CSVæ•°æ®ï¼ˆSchemaä¸è¡¨ç»“æ„ä¸¥æ ¼ä¸€è‡´ï¼‰
csv_path = "ads_product_value_evaluation_10000.csv"  # æ›¿æ¢ä¸ºå®é™…è·¯å¾„
schema = StructType([
    StructField("product_id", StringType(), nullable=False),
    StructField("product_name", StringType(), nullable=True),
    StructField("stat_date", StringType(), nullable=False),  # åˆ†åŒºå­—æ®µ
    StructField("traffic_acquisition_score", DecimalType(5, 2), nullable=True),
    StructField("traffic_conversion_score", DecimalType(5, 2), nullable=True),
    StructField("content_marketing_score", DecimalType(5, 2), nullable=True),
    StructField("customer_acquisition_score", DecimalType(5, 2), nullable=True),
    StructField("service_quality_score", DecimalType(5, 2), nullable=True),
    StructField("total_score", DecimalType(5, 2), nullable=True),
    StructField("level", StringType(), nullable=True),
    StructField("amount", DecimalType(12, 2), nullable=True),
    StructField("price", DecimalType(8, 2), nullable=True),
    StructField("sales_num", IntegerType(), nullable=True),
    StructField("visitor_num", IntegerType(), nullable=True),
    StructField("visitor_sales_ratio", DecimalType(5, 2), nullable=True)
])

df = spark.read \
    .format("csv") \
    .option("header", "true") \
    .schema(schema) \
    .load(csv_path)

# 3. å†™å…¥Hiveè¡¨ï¼ˆåŠ¨æ€åˆ†åŒºæ¨¡å¼ï¼Œç¬¦åˆæ–‡æ¡£å­˜å‚¨è¦æ±‚ï¼‰
df.write \
    .mode("append") \
    .format("hive")  \
    .partitionBy("stat_date")   \
    .saveAsTable("gd.ads_product_value_evaluation")

# 4. éªŒè¯æ•°æ®å†™å…¥ç»“æœ
count = spark.sql("SELECT COUNT(*) FROM gd.ads_product_value_evaluation").collect()[0][0]
print(f"æˆåŠŸæ’å…¥{count}æ¡æ•°æ®åˆ°gd.ads_product_value_evaluationè¡¨ï¼Œè¡¨ç»“æ„ç¬¦åˆæ–‡æ¡£å…¨å“ä»·å€¼è¯„ä¼°éœ€æ±‚")

spark.stop()